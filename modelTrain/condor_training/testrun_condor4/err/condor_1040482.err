wandb: Currently logged in as: gbrodbek (gbrodbek-kit4749). Use `wandb login --relogin` to force relogin
DGL backend not selected or invalid.  Assuming PyTorch for now.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: gbrodbek (gbrodbek-kit4749). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.2
wandb: Run data is saved locally in ./wandb/run-20241121_172809-ysuxvzpw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run condor_testrun
wandb: â­ï¸ View project at https://wandb.ai/gbrodbek-kit4749/test_logs
wandb: ğŸš€ View run at https://wandb.ai/gbrodbek-kit4749/test_logs/runs/ysuxvzpw
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /var/lib/condor/execute/dir_1439506/tmp exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-bd60eeb4]

  | Name                    | Type        | Params
--------------------------------------------------------
0 | gatr                    | GATr        | 924 K 
1 | ScaledGooeyBatchNorm2_1 | BatchNorm1d | 6     
2 | loss_crit               | BCELoss     | 0     
3 | MLP_layer               | MLPReadout  | 945   
4 | m                       | Sigmoid     | 0     
--------------------------------------------------------
925 K     Trainable params
0         Non-trainable params
925 K     Total params
3.703     Total estimated model params size (MB)
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=254` in the `DataLoader` to improve performance.
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=254` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=10` reached.
wandb: - 15.239 MB of 15.254 MB uploadedwandb: \ 15.240 MB of 15.254 MB uploadedwandb: | 15.240 MB of 15.254 MB uploadedwandb: / 15.240 MB of 15.254 MB uploadedwandb: - 15.240 MB of 15.254 MB uploadedwandb: \ 15.240 MB of 15.266 MB uploadedwandb: | 15.253 MB of 15.266 MB uploadedwandb: / 15.254 MB of 15.266 MB uploadedwandb: - 15.255 MB of 15.266 MB uploadedwandb: \ 15.255 MB of 15.266 MB uploadedwandb: | 15.256 MB of 15.266 MB uploadedwandb: / 15.257 MB of 15.266 MB uploadedwandb: - 15.257 MB of 15.266 MB uploadedwandb: \ 15.258 MB of 15.266 MB uploadedwandb: | 15.258 MB of 15.266 MB uploadedwandb: / 15.259 MB of 15.266 MB uploadedwandb: - 15.260 MB of 15.266 MB uploadedwandb: \ 15.260 MB of 15.266 MB uploadedwandb: | 15.261 MB of 15.266 MB uploadedwandb: / 15.262 MB of 15.266 MB uploadedwandb: - 15.263 MB of 15.266 MB uploadedwandb: \ 15.263 MB of 15.266 MB uploadedwandb: | 15.264 MB of 15.266 MB uploadedwandb: / 15.265 MB of 15.266 MB uploadedwandb: - 15.266 MB of 15.266 MB uploadedwandb: \ 15.266 MB of 15.266 MB uploadedwandb: 
wandb: Run history:
wandb:                       accuracy â–â–‚â–‚â–…â–‚â–„â–„â–…â–…â–…â–†â–‡â–…â–†â–‡â–‡â–†â–‡â–‡â–…â–†â–‡â–…â–‡â–…â–…â–ˆâ–‡â–…â–‡â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–‡
wandb:                  accuracy val  â–â–ƒâ–‚â–‚â–ƒâ–†â–†â–†â–…â–‡â–†â–†â–…â–‡â–‡â–‡â–„â–‚â–ƒâ–ƒâ–†â–…â–„â–†â–†â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–†â–‡â–‡â–‡â–ˆâ–‡
wandb:                          epoch â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ
wandb:                           loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb: loss_comp_time_inside_training â–‚â–â–…â–‚â–‚â–‚â–‚â–‚â–…â–…â–…â–‚â–…â–‚â–‚â–…â–†â–‡â–…â–„â–â–‚â–ˆâ–ƒâ–†â–‡â–…â–ƒâ–„â–…â–ƒâ–„â–…â–„â–…â–„â–‚â–â–„â–…
wandb:                       loss_val â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        lr-Adam â–â–â–â–â–â–â–â–â–â–
wandb:      misc_time_inside_training â–â–â–‚â–â–â–â–â–â–‚â–‚â–ˆâ–â–‚â–‚â–‚â–‚â–„â–‚â–‚â–‚â–â–â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–„â–‚â–‚â–‚â–â–â–‚â–‚
wandb:               train_loss_epoch â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–
wandb:            trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             training_step_time â–ƒâ–â–†â–‚â–â–…â–‚â–‚â–†â–†â–†â–…â–†â–â–‚â–‡â–†â–†â–…â–‡â–ƒâ–‚â–†â–ƒâ–†â–ˆâ–…â–„â–…â–†â–…â–…â–„â–…â–…â–„â–…â–…â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:                       accuracy 0.73913
wandb:                  accuracy val  0.88
wandb:                          epoch 9
wandb:                           loss 0.24582
wandb: loss_comp_time_inside_training 0.0005
wandb:                       loss_val 0.11292
wandb:                        lr-Adam 0.001
wandb:      misc_time_inside_training 0.00051
wandb:               train_loss_epoch 0.48468
wandb:            trainer/global_step 909
wandb:             training_step_time 0.13546
wandb: 
wandb: ğŸš€ View run condor_testrun at: https://wandb.ai/gbrodbek-kit4749/test_logs/runs/ysuxvzpw
wandb: Synced 5 W&B file(s), 222 media file(s), 226 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_172809-ysuxvzpw/logs
